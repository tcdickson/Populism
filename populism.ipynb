{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-14T17:11:42.434337Z",
     "start_time": "2025-08-14T17:11:41.435619Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('Final_Populism.csv')\n",
    "df = df[['Summary', 'Original_Text', 'Article_Title','Is_Populist']]\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T19:13:45.984570Z",
     "start_time": "2025-08-14T17:11:47.598796Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import os, random, numpy as np, pandas as pd, torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq,\n",
    "    TrainingArguments, Trainer\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import evaluate\n",
    "\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "MODEL_NAME = \"facebook/bart-base\"   \n",
    "MAX_SRC = 1024\n",
    "MAX_SUM = 128\n",
    "DROP_TITLE_PROB = 0.5  \n",
    "USE_LORA = True\n",
    "\n",
    "\n",
    "df['Is_Populist'] = df['Is_Populist'].astype(int).clip(0,1)\n",
    "\n",
    "def make_prompt(title, text, may_drop=False):\n",
    "    use_title = bool(title) and (not may_drop or random.random() > DROP_TITLE_PROB)\n",
    "    prefix = (title.strip() + \". \") if use_title else \"\"\n",
    "    return (prefix + (text or \"\")).strip()\n",
    "\n",
    "def row_to_examples(row, may_drop=False):\n",
    "    x_sum = \"summarize: \" + make_prompt(row['Article_Title'], row['Original_Text'], may_drop=may_drop)\n",
    "    x_cls = \"classify_populism: \" + make_prompt(row['Article_Title'], row['Original_Text'], may_drop=may_drop)\n",
    "    return [\n",
    "        {\"task\":\"summarize\", \"input\": x_sum, \"target\": row[\"Summary\"]},\n",
    "        {\"task\":\"classify\",  \"input\": x_cls, \"target\": str(int(row[\"Is_Populist\"]))}\n",
    "    ]\n",
    "\n",
    "def row_to_examples_no_title(row):\n",
    "    x_sum = \"summarize: \" + (row['Original_Text'] or \"\")\n",
    "    x_cls = \"classify_populism: \" + (row['Original_Text'] or \"\")\n",
    "    return [\n",
    "        {\"task\":\"summarize\", \"input\": x_sum, \"target\": str(row[\"Summary\"]).strip()},\n",
    "        {\"task\":\"classify\",  \"input\": x_cls, \"target\": str(int(row[\"Is_Populist\"]))}\n",
    "    ]\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['Is_Populist'])\n",
    "val_df, test_df   = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['Is_Populist'])\n",
    "\n",
    "train_examples = [ex for _, r in train_df.iterrows() for ex in row_to_examples(r, may_drop=True)]\n",
    "\n",
    "\n",
    "val_examples   = [ex for _, r in val_df.iterrows()  for ex in row_to_examples_no_title(r)]\n",
    "test_examples  = [ex for _, r in test_df.iterrows() for ex in row_to_examples_no_title(r)]\n",
    "\n",
    "train_ds = Dataset.from_list(train_examples)\n",
    "val_ds   = Dataset.from_list(val_examples)\n",
    "test_ds  = Dataset.from_list(test_examples)\n",
    "\n",
    "val_sum  = val_ds.filter(lambda ex: ex[\"task\"]==\"summarize\")\n",
    "val_cls  = val_ds.filter(lambda ex: ex[\"task\"]==\"classify\")\n",
    "test_sum = test_ds.filter(lambda ex: ex[\"task\"]==\"summarize\")\n",
    "test_cls = test_ds.filter(lambda ex: ex[\"task\"]==\"classify\")\n",
    "\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "tok.padding_side = \"right\"\n",
    "if tok.pad_token_id is None:\n",
    "    tok.pad_token = tok.eos_token  \n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    ins = tok(\n",
    "        batch[\"input\"], truncation=True, max_length=MAX_SRC, padding=\"max_length\",\n",
    "    )\n",
    "    labs = tok(\n",
    "        text_target=batch[\"target\"], truncation=True, max_length=MAX_SUM, padding=\"max_length\",\n",
    "    )\n",
    "    ins[\"labels\"] = labs[\"input_ids\"]    # collator will convert pad -> -100\n",
    "    return ins\n",
    "\n",
    "\n",
    "train_tok = train_ds.map(tokenize_fn, batched=True, remove_columns=train_ds.column_names)\n",
    "val_sum_tok  = val_sum.map(tokenize_fn, batched=True, remove_columns=val_sum.column_names)\n",
    "val_cls_tok  = val_cls.map(tokenize_fn, batched=True, remove_columns=val_cls.column_names)\n",
    "test_sum_tok = test_sum.map(tokenize_fn, batched=True, remove_columns=test_sum.column_names)\n",
    "test_cls_tok = test_cls.map(tokenize_fn, batched=True, remove_columns=test_cls.column_names)\n",
    "\n",
    "\n",
    "base = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float32,       \n",
    "    attn_implementation=\"eager\"\n",
    ").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# model = get_peft_model(base, LoraConfig(task_type=\"SEQ_2_SEQ_LM\", r=16, lora_alpha=32, lora_dropout=0.05)) if USE_LORA else base\n",
    "\n",
    "lora_cfg = LoraConfig(\n",
    "    task_type=\"SEQ_2_SEQ_LM\",\n",
    "    r=16, lora_alpha=32, lora_dropout=0.05, bias=\"none\",\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"out_proj\",\"fc1\",\"fc2\"],\n",
    "    modules_to_save=[\"lm_head\"], \n",
    ")\n",
    "model = get_peft_model(base, lora_cfg)\n",
    "\n",
    "collator = DataCollatorForSeq2Seq(tokenizer=tok, model=model, return_tensors=\"pt\")\n",
    "        \n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model.config.use_cache = False\n",
    "model.train()\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "acc   = evaluate.load(\"accuracy\"); prec = evaluate.load(\"precision\")\n",
    "rec   = evaluate.load(\"recall\");   f1   = evaluate.load(\"f1\")\n",
    "\n",
    "def _decode(pred_ids, label_ids):\n",
    "    pred_ids  = np.where(pred_ids  != -100, pred_ids,  tok.pad_token_id)\n",
    "    label_ids = np.where(label_ids != -100, label_ids, tok.pad_token_id)\n",
    "    preds  = tok.batch_decode(pred_ids,  skip_special_tokens=True)\n",
    "    labels = tok.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    return preds, labels\n",
    "\n",
    "def summarize_metrics(eval_pred):\n",
    "    pred_ids, label_ids = eval_pred\n",
    "    preds, labels = _decode(pred_ids, label_ids)\n",
    "    r = rouge.compute(predictions=preds, references=labels, use_stemmer=True)\n",
    "    return {f\"rouge_{k}\": v for k,v in r.items()}\n",
    "\n",
    "def classify_metrics(eval_pred):\n",
    "    pred_ids, label_ids = eval_pred\n",
    "    preds, labels = _decode(pred_ids, label_ids)\n",
    "    preds_bin  = [1 if (p.strip() and p.strip()[0]=='1') else 0 for p in preds]\n",
    "    labels_bin = [1 if (l.strip() and l.strip()[0]=='1') else 0 for l in labels]\n",
    "    return {\n",
    "        \"accuracy\":  acc.compute(predictions=preds_bin, references=labels_bin)[\"accuracy\"],\n",
    "        \"precision\": prec.compute(predictions=preds_bin, references=labels_bin)[\"precision\"],\n",
    "        \"recall\":    rec.compute(predictions=preds_bin, references=labels_bin)[\"recall\"],\n",
    "        \"f1\":        f1.compute(predictions=preds_bin, references=labels_bin)[\"f1\"],\n",
    "    }\n",
    "\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"bart_multitask_lora\",\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    eval_accumulation_steps=32,\n",
    "    gradient_accumulation_steps=4,\n",
    "\n",
    "    fp16=True, bf16=False,            \n",
    "    max_grad_norm=1.0,\n",
    "\n",
    "    # evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=False,\n",
    "    metric_for_best_model=\"eval_rougeLsum\",\n",
    "    greater_is_better=True,\n",
    "\n",
    "    predict_with_generate=True,\n",
    "    generation_num_beams=5,\n",
    "    # generation_max_new_tokens=MAX_SUM,\n",
    "\n",
    "    logging_steps=50,\n",
    "    dataloader_pin_memory=True,\n",
    "    gradient_checkpointing=True,\n",
    "    # optim=\"adamw_bnb_8bit\",\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    report_to=\"none\",\n",
    "    torch_compile=False,\n",
    "    dataloader_num_workers=2,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=val_sum_tok,\n",
    "    processing_class=tok,      # <- use this instead of tokenizer=tok\n",
    "    data_collator=collator,\n",
    "    compute_metrics=summarize_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ],
   "id": "ee83fa1e16229402",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Filter:   0%|          | 0/1980 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "694847fa45d748a6bdd553eeb6c2604e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Filter:   0%|          | 0/1980 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3e8f62637bae490f9ba118a1e9fff8c4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Filter:   0%|          | 0/1980 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f39a31a1f0494d2e932611bd1cecc57a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Filter:   0%|          | 0/1980 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "41d9073d43074c82bd163adfec129f15"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/15834 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "176e81dc72334ac18628696a2e1e764a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/990 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bf6d0775836642468780d4162973c9bf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/990 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "04f5236e55fb4733bda76e55a0960385"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/990 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "92a72bb971ea44f79fbebce79fc2b194"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/990 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ab104038f14d432e91ee25102f954e5a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tyty/anaconda3/envs/Palantir_Project/lib/python3.11/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/tyty/anaconda3/envs/Palantir_Project/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='744' max='744' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [744/744 2:01:15, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>10.116100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>6.163000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3.086800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.648200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.315600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.179600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.164200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.144200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.129000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.128800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.079900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.086100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.113800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.049600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tyty/anaconda3/envs/Palantir_Project/lib/python3.11/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/tyty/anaconda3/envs/Palantir_Project/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/tyty/anaconda3/envs/Palantir_Project/lib/python3.11/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/tyty/anaconda3/envs/Palantir_Project/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=744, training_loss=2.2419506349871234, metrics={'train_runtime': 7286.2738, 'train_samples_per_second': 6.519, 'train_steps_per_second': 0.102, 'total_flos': 4.117700503771546e+16, 'train_loss': 2.2419506349871234, 'epoch': 3.0})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3419351ae6b9eca5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T21:34:18.182582Z",
     "start_time": "2025-08-14T20:08:27.257223Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers.utils import logging\n",
    "logging.set_verbosity_error()  \n",
    "\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sum_gc = GenerationConfig(\n",
    "    do_sample=False, num_beams=5,\n",
    "    max_new_tokens=MAX_SUM,\n",
    "    length_penalty=1.1,         \n",
    "    no_repeat_ngram_size=3,     \n",
    "    min_new_tokens=16            \n",
    ")\n",
    "\n",
    "\n",
    "cls_gc = GenerationConfig(do_sample=False, num_beams=1, max_new_tokens=2)\n",
    "\n",
    "trainer.compute_metrics = summarize_metrics\n",
    "\n",
    "model.generation_config = sum_gc\n",
    "print(\"Validation – Summarization (no-title):\",\n",
    "      trainer.evaluate(eval_dataset=val_sum_tok, metric_key_prefix=\"sum_val\"))\n",
    "\n",
    "print(\"Test – Summarization (no-title):\",\n",
    "      trainer.evaluate(eval_dataset=test_sum_tok, metric_key_prefix=\"sum_test\"))\n",
    "\n",
    "trainer.compute_metrics = classify_metrics\n",
    "pred_val = trainer.predict(val_cls_tok,  metric_key_prefix=\"cls_val\",  generation_config=cls_gc)\n",
    "print(\"Validation – Classification (no-title):\",\n",
    "      classify_metrics((pred_val.predictions, pred_val.label_ids)))\n",
    "\n",
    "pred_test = trainer.predict(test_cls_tok, metric_key_prefix=\"cls_test\", generation_config=cls_gc)\n",
    "print(\"Test – Classification (no-title):\",\n",
    "      classify_metrics((pred_test.predictions, pred_test.label_ids)))\n",
    "\n",
    "import torch\n",
    "@torch.no_grad()\n",
    "def predict_summary_and_label(text: str):\n",
    "    device = next(model.parameters()).device\n",
    "    x = text.strip()\n",
    "\n",
    "    in_sum = tok(\"summarize: \" + x, return_tensors=\"pt\",\n",
    "                 truncation=True, max_length=MAX_SRC).to(device)\n",
    "    out_sum = model.generate(**in_sum)\n",
    "    summary = tok.decode(out_sum[0], skip_special_tokens=True).strip()\n",
    "\n",
    "    in_cls = tok(\"classify_populism: \" + x, return_tensors=\"pt\",\n",
    "                 truncation=True, max_length=MAX_SRC).to(device)\n",
    "    out_cls = model.generate(**in_cls, max_new_tokens=2, do_sample=False)\n",
    "    lab = tok.decode(out_cls[0], skip_special_tokens=True).strip()\n",
    "    is_pop = 1 if lab.lstrip().startswith(\"1\") else 0\n",
    "\n",
    "    return summary, is_pop\n",
    "\n",
    "\n",
    "trainer.save_model(\"final_adapter\")  \n",
    "tok.save_pretrained(\"final_adapter\")\n",
    "\n",
    "if USE_LORA:\n",
    "    merged = model.merge_and_unload()\n",
    "else:\n",
    "    merged = model\n",
    "merged.save_pretrained(\"final_merged\", safe_serialization=True)\n",
    "tok.save_pretrained(\"final_merged\")\n"
   ],
   "id": "8c4ce1437915f07a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tyty/anaconda3/envs/Palantir_Project/lib/python3.11/site-packages/transformers/generation/utils.py:1738: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed in v5. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "/home/tyty/anaconda3/envs/Palantir_Project/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation – Summarization (no-title): {'sum_val_loss': 1.8238072395324707, 'sum_val_rouge_rouge1': 0.20916817359746792, 'sum_val_rouge_rouge2': 0.07701058107896677, 'sum_val_rouge_rougeL': 0.17252977570196587, 'sum_val_rouge_rougeLsum': 0.17254445260503692, 'sum_val_runtime': 2217.5431, 'sum_val_samples_per_second': 0.446, 'sum_val_steps_per_second': 0.028, 'epoch': 3.0}\n",
      "Test – Summarization (no-title): {'sum_test_loss': 1.8614931106567383, 'sum_test_rouge_rouge1': 0.2075064122017521, 'sum_test_rouge_rouge2': 0.07582501786662132, 'sum_test_rouge_rougeL': 0.1708362966991224, 'sum_test_rouge_rougeLsum': 0.170730714017154, 'sum_test_runtime': 2158.1586, 'sum_test_samples_per_second': 0.459, 'sum_test_steps_per_second': 0.029, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tyty/anaconda3/envs/Palantir_Project/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation – Classification (no-title): {'accuracy': 0.5969696969696969, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tyty/anaconda3/envs/Palantir_Project/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/tyty/anaconda3/envs/Palantir_Project/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tyty/anaconda3/envs/Palantir_Project/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/tyty/anaconda3/envs/Palantir_Project/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test – Classification (no-title): {'accuracy': 0.5969696969696969, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tyty/anaconda3/envs/Palantir_Project/lib/python3.11/site-packages/transformers/modeling_utils.py:3909: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('final_merged/tokenizer_config.json',\n",
       " 'final_merged/special_tokens_map.json',\n",
       " 'final_merged/vocab.json',\n",
       " 'final_merged/merges.txt',\n",
       " 'final_merged/added_tokens.json',\n",
       " 'final_merged/tokenizer.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "22f6bf6c37ec70d0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T23:07:25.588922Z",
     "start_time": "2025-08-13T23:07:24.370290Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, GenerationConfig\n",
    "\n",
    "MODEL_DIR = \"final_merged\"\n",
    "MAX_SRC = 500\n",
    "MAX_SUM = 128\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_DIR, use_fast=True)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_DIR,\n",
    "    torch_dtype=(torch.float16 if torch.cuda.is_available() else torch.float32),\n",
    "    low_cpu_mem_usage=True,\n",
    ").to(device).eval()\n",
    "\n",
    "try:\n",
    "    model.generation_config = GenerationConfig.from_pretrained(MODEL_DIR)\n",
    "except Exception:\n",
    "    model.generation_config = GenerationConfig(do_sample=False, num_beams=1, max_new_tokens=MAX_SUM)\n",
    "\n",
    "def _to_str(x):\n",
    "    if isinstance(x, str):\n",
    "        return x\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        return \" \".join(map(str, x))\n",
    "    try:\n",
    "        return str(x)\n",
    "    except Exception:\n",
    "        return \" \".join(map(str, x))\n",
    "\n",
    "# --- Quick test ---\n",
    "text = \"Labour's Séan Sherlock has called for Minister of State Pat Breen to clarify his meetings with David McCourt, a lead bidder on the National Broadband Plan. Allegations of undisclosed meetings, including private visits, have raised concerns about transparency and potential conflicts of interest. Labour demands Fine Gael provide full disclosure on these interactions.\",\"Labour Dáil Communications spokesperson Séan Sherlock has called on Minister of State Pat Breen to make a statement to the Dáil outlining the purpose and content of his meetings with David McCourt following further reports of meetings in the Irish Mail on Sunday, and Sunday Business Post.Labour raised the possibility of Minister of State Breen making a Dáil statement at last Thursday's Business Committee meeting, and will pursue it further next week.Deputy Sherlock said: There is a drip-feed of revelations about the meetings between both the former Minister for Communications with David McCourt, but also the many meetings that have taken place with Minister of State Pat Breen. The Mail on Sunday yesterday revealed that Minister Breen met with Mr McCourt for breakfast before the bidder went on to meet the Minister for Communications in 2016. The Sunday Business Post also revealed there were three previously unreported meetings, with visits on several occasions to the bidder's home in a 'private capacity'. It is now necessary for the Minister of State to make a clear statement to the Dáil, outlining the purposes of all these meetings, and the topics of discussions. It is simply not believable that the National Broadband Plan would not have come up at all considering it is a major state contract that Mr McCourt is leading the bid on. It is time for Fine Gael to come clean on all the meetings that have taken place with the lead bidder on the tender. The Labour Party raised at Thursday's Business Committee the possibility of the Minister making a statement to the Dáil on the matter. Following Sunday's revelations, the Minister must now clarify the many meetings and the nature of the discussions at them.,Breen should address Dáil - FG must come clean on McCourt meetings\"\n",
    "print(\"text type:\", type(text))  # sanity check\n",
    "summary, is_pop = predict_summary_and_label(text)\n",
    "print(\"SUMMARY:\\n\", summary)\n",
    "print(\"IS_POPULIST:\", is_pop)\n"
   ],
   "id": "2b4205ed4fc5723",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text type: <class 'tuple'>\n",
      "SUMMARY:\n",
      " Labour's Séan Sherlock calls for Minister of State Pat Breen to clarify his meetings\n",
      "IS_POPULIST: 0\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "271f7e30c881744c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
