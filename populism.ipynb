{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-15T15:19:05.145761Z",
     "start_time": "2025-08-15T15:19:04.132319Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('Final_Populism.csv')\n",
    "df = df[['Summary', 'Original_Text', 'Article_Title','Is_Populist']]\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T01:34:32.936552Z",
     "start_time": "2025-08-15T15:19:05.146829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import os, random, numpy as np, pandas as pd, torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq,\n",
    "    TrainingArguments, Trainer\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import evaluate\n",
    "\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "MODEL_NAME = \"facebook/bart-base\"   \n",
    "MAX_SRC = 1024\n",
    "MAX_SUM = 128\n",
    "DROP_TITLE_PROB = 0.5  \n",
    "USE_LORA = True\n",
    "\n",
    "\n",
    "df['Is_Populist'] = df['Is_Populist'].astype(int).clip(0,1)\n",
    "\n",
    "def make_prompt(title, text, may_drop=False):\n",
    "    use_title = bool(title) and (not may_drop or random.random() > DROP_TITLE_PROB)\n",
    "    prefix = (title.strip() + \". \") if use_title else \"\"\n",
    "    return (prefix + (text or \"\")).strip()\n",
    "\n",
    "def row_to_examples(row, may_drop=False, upsample_pos=True, pos_factor=2):\n",
    "    exs = []\n",
    "\n",
    "    x_sum = \"summarize: \" + make_prompt(row['Article_Title'], row['Original_Text'], may_drop=may_drop)\n",
    "    exs.append({\"task\":\"summarize\", \"input\": x_sum, \"target\": str(row[\"Summary\"]).strip()})\n",
    "\n",
    "    x_cls = \"classify_populism: \" + make_prompt(row['Article_Title'], row['Original_Text'], may_drop=may_drop)\n",
    "    y = int(row[\"Is_Populist\"])\n",
    "    cls_ex = {\"task\":\"classify\", \"input\": x_cls, \"target\": str(y)}\n",
    "    exs.append(cls_ex)\n",
    "\n",
    "    if upsample_pos and y == 1 and pos_factor > 1:\n",
    "        for _ in range(pos_factor - 1):\n",
    "            exs.append(cls_ex.copy())\n",
    "\n",
    "    return exs\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['Is_Populist'])\n",
    "val_df, test_df   = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['Is_Populist'])\n",
    "\n",
    "# TRAIN: title-dropout + positive upsampling\n",
    "train_examples = [ex\n",
    "    for _, r in train_df.iterrows()\n",
    "    for ex in row_to_examples(r, may_drop=True, upsample_pos=True, pos_factor=2)  # try 2; tune to 3–4 if recall still 0\n",
    "]\n",
    "\n",
    "# VAL/TEST: NO TITLE and NO upsampling (mirror deployment)\n",
    "def row_to_examples_no_title(row):\n",
    "    return [\n",
    "        {\"task\":\"summarize\", \"input\": \"summarize: \" + (row['Original_Text'] or \"\"), \"target\": str(row[\"Summary\"]).strip()},\n",
    "        {\"task\":\"classify\",  \"input\": \"classify_populism: \" + (row['Original_Text'] or \"\"), \"target\": str(int(row[\"Is_Populist\"]))}\n",
    "    ]\n",
    "\n",
    "val_examples  = [ex for _, r in val_df.iterrows()  for ex in row_to_examples_no_title(r)]\n",
    "test_examples = [ex for _, r in test_df.iterrows() for ex in row_to_examples_no_title(r)]\n",
    "\n",
    "\n",
    "train_ds = Dataset.from_list(train_examples)\n",
    "val_ds   = Dataset.from_list(val_examples)\n",
    "test_ds  = Dataset.from_list(test_examples)\n",
    "\n",
    "val_sum  = val_ds.filter(lambda ex: ex[\"task\"]==\"summarize\")\n",
    "val_cls  = val_ds.filter(lambda ex: ex[\"task\"]==\"classify\")\n",
    "test_sum = test_ds.filter(lambda ex: ex[\"task\"]==\"summarize\")\n",
    "test_cls = test_ds.filter(lambda ex: ex[\"task\"]==\"classify\")\n",
    "\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "tok.padding_side = \"right\"\n",
    "if tok.pad_token_id is None:\n",
    "    tok.pad_token = tok.eos_token  \n",
    "\n",
    "\n",
    "base = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float32,       \n",
    "    attn_implementation=\"eager\"\n",
    ").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# model = get_peft_model(base, LoraConfig(task_type=\"SEQ_2_SEQ_LM\", r=16, lora_alpha=32, lora_dropout=0.05)) if USE_LORA else base\n",
    "\n",
    "lora_cfg = LoraConfig(\n",
    "    task_type=\"SEQ_2_SEQ_LM\",\n",
    "    r=16, lora_alpha=32, lora_dropout=0.05, bias=\"none\",\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"out_proj\",\"fc1\",\"fc2\"],\n",
    "    modules_to_save=[\"lm_head\"], \n",
    ")\n",
    "\n",
    "model = get_peft_model(base, lora_cfg)\n",
    "\n",
    "# collator = DataCollatorForSeq2Seq(tokenizer=tok, model=model, return_tensors=\"pt\")\n",
    "        \n",
    "def tokenize_fn(batch):\n",
    "    ins = tok(\n",
    "        batch[\"input\"],\n",
    "        truncation=True,\n",
    "        max_length=MAX_SRC,\n",
    "        padding=False,                 # <— was \"max_length\"\n",
    "    )\n",
    "    labs = tok(\n",
    "        text_target=batch[\"target\"],\n",
    "        truncation=True,\n",
    "        max_length=MAX_SUM,\n",
    "        padding=False,                 # <— was \"max_length\"\n",
    "    )\n",
    "    ins[\"labels\"] = labs[\"input_ids\"]\n",
    "    return ins\n",
    "\n",
    "collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tok,\n",
    "    model=model,\n",
    "    return_tensors=\"pt\",\n",
    "    pad_to_multiple_of=8               # <— keeps fp16-friendly padding\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "train_tok = train_ds.map(tokenize_fn, batched=True, remove_columns=train_ds.column_names)\n",
    "val_sum_tok  = val_sum.map(tokenize_fn, batched=True, remove_columns=val_sum.column_names)\n",
    "val_cls_tok  = val_cls.map(tokenize_fn, batched=True, remove_columns=val_cls.column_names)\n",
    "test_sum_tok = test_sum.map(tokenize_fn, batched=True, remove_columns=test_sum.column_names)\n",
    "test_cls_tok = test_cls.map(tokenize_fn, batched=True, remove_columns=test_cls.column_names)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model.config.use_cache = False\n",
    "model.train()\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "acc   = evaluate.load(\"accuracy\"); prec = evaluate.load(\"precision\")\n",
    "rec   = evaluate.load(\"recall\");   f1   = evaluate.load(\"f1\")\n",
    "\n",
    "def _decode(pred_ids, label_ids):\n",
    "    pred_ids  = np.where(pred_ids  != -100, pred_ids,  tok.pad_token_id)\n",
    "    label_ids = np.where(label_ids != -100, label_ids, tok.pad_token_id)\n",
    "    preds  = tok.batch_decode(pred_ids,  skip_special_tokens=True)\n",
    "    labels = tok.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    return preds, labels\n",
    "\n",
    "def summarize_metrics(eval_pred):\n",
    "    pred_ids, label_ids = eval_pred\n",
    "    preds, labels = _decode(pred_ids, label_ids)\n",
    "    r = rouge.compute(predictions=preds, references=labels, use_stemmer=True)\n",
    "    return {f\"rouge_{k}\": v for k,v in r.items()}\n",
    "\n",
    "def classify_metrics(eval_pred):\n",
    "    pred_ids, label_ids = eval_pred\n",
    "    preds, labels = _decode(pred_ids, label_ids)\n",
    "    preds_bin  = [1 if (p.strip() and p.strip()[0]=='1') else 0 for p in preds]\n",
    "    labels_bin = [1 if (l.strip() and l.strip()[0]=='1') else 0 for l in labels]\n",
    "    return {\n",
    "        \"accuracy\":  acc.compute(predictions=preds_bin, references=labels_bin)[\"accuracy\"],\n",
    "        \"precision\": prec.compute(predictions=preds_bin, references=labels_bin)[\"precision\"],\n",
    "        \"recall\":    rec.compute(predictions=preds_bin, references=labels_bin)[\"recall\"],\n",
    "        \"f1\":        f1.compute(predictions=preds_bin, references=labels_bin)[\"f1\"],\n",
    "    }\n",
    "\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"bart_multitask_lora\",\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    eval_accumulation_steps=32,\n",
    "    gradient_accumulation_steps=4,\n",
    "    weight_decay=0.01,\n",
    "\n",
    "    fp16=True, bf16=False,            \n",
    "    max_grad_norm=1.0,\n",
    "    label_smoothing_factor=0.1,\n",
    "\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_rouge_rougeLsum\",\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=2,\n",
    "\n",
    "    predict_with_generate=True,\n",
    "    generation_num_beams=3,\n",
    "    # generation_max_new_tokens=MAX_SUM,\n",
    "    lr_scheduler_type=\"cosine\",      \n",
    "    warmup_ratio=0.06,\n",
    "    generation_max_length=MAX_SUM,\n",
    "\n",
    "    logging_steps=50,\n",
    "    dataloader_pin_memory=True,\n",
    "    gradient_checkpointing=True,\n",
    "    # optim=\"adamw_bnb_8bit\",\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    report_to=\"none\",\n",
    "    torch_compile=False,\n",
    "    dataloader_num_workers=2,\n",
    "    group_by_length=True\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=val_sum_tok,\n",
    "    # processing_class=tok,    \n",
    "    tokenizer=tok,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=summarize_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "print(\"Best metric:\", trainer.state.best_metric)\n",
    "print(\"Best checkpoint:\", trainer.state.best_model_checkpoint)"
   ],
   "id": "ee83fa1e16229402",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Filter:   0%|          | 0/1980 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "52bfd21b4316444585cf53002f592f03"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Filter:   0%|          | 0/1980 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bdf698bf33bd4ca0aa2b25fab2c666af"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Filter:   0%|          | 0/1980 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a12c30df3ce642b984be194c7cf6728c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Filter:   0%|          | 0/1980 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "56dcae99237b401a9be235108de1922b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/19023 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "193ca76b3ccd48dfb6d93950610dc647"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/990 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "481cba26496d4c96a892e5eebf427372"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/990 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ef78ce7125104dfe80a7953f52ebb2de"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/990 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "af89a669debb486992821e00f5dfa053"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/990 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a374c47f95a94969886be273eef32357"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1907270/3307407927.py:225: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "/home/tyty/anaconda3/envs/Palantir_Project/lib/python3.11/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1192' max='1192' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1192/1192 10:14:37, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge Rouge1</th>\n",
       "      <th>Rouge Rouge2</th>\n",
       "      <th>Rouge Rougel</th>\n",
       "      <th>Rouge Rougelsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.680900</td>\n",
       "      <td>4.267630</td>\n",
       "      <td>0.219664</td>\n",
       "      <td>0.071383</td>\n",
       "      <td>0.159296</td>\n",
       "      <td>0.159171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.364500</td>\n",
       "      <td>4.132442</td>\n",
       "      <td>0.298937</td>\n",
       "      <td>0.093362</td>\n",
       "      <td>0.212480</td>\n",
       "      <td>0.212500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.258400</td>\n",
       "      <td>4.094854</td>\n",
       "      <td>0.306981</td>\n",
       "      <td>0.099511</td>\n",
       "      <td>0.216608</td>\n",
       "      <td>0.216348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.219100</td>\n",
       "      <td>4.088968</td>\n",
       "      <td>0.306655</td>\n",
       "      <td>0.098356</td>\n",
       "      <td>0.216579</td>\n",
       "      <td>0.216506</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tyty/anaconda3/envs/Palantir_Project/lib/python3.11/site-packages/transformers/generation/utils.py:1733: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed in v5. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "/home/tyty/anaconda3/envs/Palantir_Project/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/tyty/anaconda3/envs/Palantir_Project/lib/python3.11/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/tyty/anaconda3/envs/Palantir_Project/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/tyty/anaconda3/envs/Palantir_Project/lib/python3.11/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/tyty/anaconda3/envs/Palantir_Project/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/tyty/anaconda3/envs/Palantir_Project/lib/python3.11/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/tyty/anaconda3/envs/Palantir_Project/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best metric: 0.21650597924602138\n",
      "Best checkpoint: bart_multitask_lora/checkpoint-1192\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T20:06:39.229330Z",
     "start_time": "2025-08-16T16:34:13.489840Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers.utils import logging\n",
    "logging.set_verbosity_error()  \n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from transformers import GenerationConfig\n",
    "sum_gc = GenerationConfig(\n",
    "    do_sample=False, num_beams=5,\n",
    "    max_new_tokens=MAX_SUM,\n",
    "    length_penalty=1.1,         \n",
    "    no_repeat_ngram_size=3,     \n",
    "    min_new_tokens=16            \n",
    ")\n",
    "\n",
    "\n",
    "cls_gc = GenerationConfig(do_sample=False, num_beams=1, max_new_tokens=2)\n",
    "\n",
    "trainer.compute_metrics = summarize_metrics\n",
    "\n",
    "print(\"Validation – Summarization (no-title):\",\n",
    "      trainer.evaluate(val_sum_tok,  metric_key_prefix=\"sum_val\",  generation_config=sum_gc))\n",
    "print(\"Test – Summarization (no-title):\",\n",
    "      trainer.evaluate(test_sum_tok, metric_key_prefix=\"sum_test\", generation_config=sum_gc))\n",
    "\n",
    "\n",
    "def eval_cls_argmax_first_token(ds, model, tok, collator, batch_size=32):\n",
    "    device = next(model.parameters()).device\n",
    "    id0 = tok(\"0\", add_special_tokens=False)[\"input_ids\"][0]\n",
    "    id1 = tok(\"1\", add_special_tokens=False)[\"input_ids\"][0]\n",
    "    dec_start = model.config.decoder_start_token_id\n",
    "\n",
    "    dl = DataLoader(ds, batch_size=batch_size, shuffle=False, collate_fn=collator)\n",
    "    preds, refs = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in dl:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attn      = batch[\"attention_mask\"].to(device)\n",
    "            dec_inp   = torch.full((input_ids.size(0), 1), dec_start, dtype=torch.long, device=device)\n",
    "            logits    = model(input_ids=input_ids, attention_mask=attn,\n",
    "                              decoder_input_ids=dec_inp, use_cache=False).logits[:, -1, :]\n",
    "            pred = (logits[:, id1] > logits[:, id0]).long().cpu().tolist()\n",
    "            preds.extend(pred)\n",
    "\n",
    "            lab0 = batch[\"labels\"][:, 0].clone()\n",
    "            lab0[lab0 == -100] = tok.pad_token_id\n",
    "            ref = (lab0 == id1).long().cpu().tolist()\n",
    "            refs.extend(ref)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\":  accuracy_score(refs, preds),\n",
    "        \"precision\": precision_score(refs, preds, zero_division=0),\n",
    "        \"recall\":    recall_score(refs, preds, zero_division=0),\n",
    "        \"f1\":        f1_score(refs, preds, zero_division=0),\n",
    "        \"pred_pos_rate\": float(sum(preds))/len(preds)\n",
    "    }\n",
    "\n",
    "print(\"Validation – Classification (no-title):\", eval_cls_argmax_first_token(val_cls_tok,  model, tok, collator))\n",
    "print(\"Test – Classification (no-title):\",       eval_cls_argmax_first_token(test_cls_tok, model, tok, collator))\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_summary_and_label(text: str):\n",
    "    device = next(model.parameters()).device\n",
    "    x = text.strip()\n",
    "\n",
    "    # summarization\n",
    "    in_sum = tok(\"summarize: \" + x, return_tensors=\"pt\",\n",
    "                 truncation=True, max_length=MAX_SRC).to(device)\n",
    "    out_sum = model.generate(**in_sum, num_beams=5, max_new_tokens=MAX_SUM, do_sample=False)\n",
    "    summary = tok.decode(out_sum[0], skip_special_tokens=True).strip()\n",
    "\n",
    "    # classification: first-token argmax\n",
    "    in_cls = tok(\"classify_populism: \" + x, return_tensors=\"pt\",\n",
    "                 truncation=True, max_length=MAX_SRC).to(device)\n",
    "    dec_start = model.config.decoder_start_token_id\n",
    "    dec_inp = torch.tensor([[dec_start]], device=device)\n",
    "    logits = model(**in_cls, decoder_input_ids=dec_inp, use_cache=False).logits[:, -1, :]\n",
    "    id0 = tok(\"0\", add_special_tokens=False)[\"input_ids\"][0]\n",
    "    id1 = tok(\"1\", add_special_tokens=False)[\"input_ids\"][0]\n",
    "    is_pop = int((logits[0, id1] > logits[0, id0]).item())\n",
    "\n",
    "    return summary, is_pop\n",
    "\n",
    "trainer.save_model(\"final_adapter\")  \n",
    "tok.save_pretrained(\"final_adapter\")\n",
    "\n",
    "if USE_LORA:\n",
    "    merged = model.merge_and_unload()\n",
    "else:\n",
    "    merged = model\n",
    "merged.save_pretrained(\"final_merged\", safe_serialization=True)\n",
    "tok.save_pretrained(\"final_merged\")\n"
   ],
   "id": "8c4ce1437915f07a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tyty/anaconda3/envs/Palantir_Project/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='248' max='124' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [124/124 3:27:36]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation – Summarization (no-title): {'sum_val_loss': 4.089061260223389, 'sum_val_rouge_rouge1': 0.30888842019002904, 'sum_val_rouge_rouge2': 0.09873294813504559, 'sum_val_rouge_rougeL': 0.21685772552524682, 'sum_val_rouge_rougeLsum': 0.21691444485423914, 'sum_val_runtime': 6265.6342, 'sum_val_samples_per_second': 0.158, 'sum_val_steps_per_second': 0.02, 'epoch': 4.0}\n",
      "Test – Summarization (no-title): {'sum_test_loss': 4.069072723388672, 'sum_test_rouge_rouge1': 0.3086700392636904, 'sum_test_rouge_rouge2': 0.0965638598713156, 'sum_test_rouge_rougeL': 0.21795977488581375, 'sum_test_rouge_rougeLsum': 0.21797753855264893, 'sum_test_runtime': 6249.2813, 'sum_test_samples_per_second': 0.158, 'sum_test_steps_per_second': 0.02, 'epoch': 4.0}\n",
      "Validation – Classification (no-title): {'accuracy': 0.12828282828282828, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'pred_pos_rate': 0.8717171717171717}\n",
      "Test – Classification (no-title): {'accuracy': 0.1191919191919192, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'pred_pos_rate': 0.8808080808080808}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tyty/anaconda3/envs/Palantir_Project/lib/python3.11/site-packages/transformers/modeling_utils.py:3917: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('final_merged/tokenizer_config.json',\n",
       " 'final_merged/special_tokens_map.json',\n",
       " 'final_merged/vocab.json',\n",
       " 'final_merged/merges.txt',\n",
       " 'final_merged/added_tokens.json',\n",
       " 'final_merged/tokenizer.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "22f6bf6c37ec70d0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "271f7e30c881744c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
